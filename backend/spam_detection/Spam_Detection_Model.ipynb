{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spam vs. Ham: Building a Spam Detection System üìß\n",
        "\n",
        "Hi there! üëã In this notebook, we're going to build a Machine Learning model to detect spam emails. \n",
        "We'll start by exploring our dataset, visualizing the data to understand the patterns, and then we'll clean the text data. \n",
        "Finally, we'll train a couple of models and see how well they perform. Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Load Data üìö\n",
        "First things first, we need to import the necessary tools. We'll be using `pandas` for data manipulation, `matplotlib` and `seaborn` for visualization, and `sklearn` for the magic (machine learning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setting plot styles for better aesthetics\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's load our dataset. We have a file named `spam_emails_data.csv`. Let's peek inside."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('artifacts/spam_emails_data.csv')\n",
        "\n",
        "# Display the first few rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Exploratory Data Analysis (EDA) üîç\n",
        "Before we dive into modeling, it's crucial to understand our data. Let's look at the shape of the data and check for any missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check dataset shape\n",
        "print(f\"Dataset Shape: {df.shape}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Check for duplicates\n",
        "print(\"\\nDuplicate Entries:\", df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove duplicates to avoid biasing our model.\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(f\"Shape after removing duplicates: {df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing the Ham / Spam Emails\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count of Spam vs Ham\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='label', data=df, palette='viridis')\n",
        "plt.title('Distribution of Spam vs. Ham Emails', fontsize=16)\n",
        "plt.xlabel('Label', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Engineering: Message Length\n",
        "I'm curious: do spam emails tend to be longer or shorter than regular emails? Let's create a new feature `message_length` and find out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['message_length'] = df['text'].apply(len)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting the length distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(data=df, x='message_length', hue='label', element='step', stat='density', common_norm=False)\n",
        "plt.title('Message Length Distribution by Label', fontsize=16)\n",
        "plt.xlabel('Message Length', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Word Cloud Visualization ‚òÅÔ∏è\n",
        "Let's visualize the most common words in Spam and Ham emails using WordClouds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# Combine all text for Spam and Ham\n",
        "spam_text = \" \".join(df[df['label'] == 'Spam']['text'])\n",
        "ham_text = \" \".join(df[df['label'] == 'Ham']['text'])\n",
        "\n",
        "# Create WordClouds\n",
        "wc_spam = WordCloud(width=800, height=400, background_color='black', colormap='Reds').generate(spam_text)\n",
        "wc_ham = WordCloud(width=800, height=400, background_color='white', colormap='Greens').generate(ham_text)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(wc_spam, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Spam Email Word Cloud', fontsize=16)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(wc_ham, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Ham Email Word Cloud', fontsize=16)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Text Preprocessing üßπ\n",
        "Computers understand numbers, not words. But before we convert text to numbers, we need to clean it up. \n",
        "We will:\n",
        "1. Convert to lowercase.\n",
        "2. Remove punctuation and special characters.\n",
        "3. Remove stopwords (common words like 'the', 'is', etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Try to download stopwords if not present\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove punctuation\n",
        "    nopunc = [char for char in text if char not in string.punctuation]\n",
        "    nopunc = ''.join(nopunc)\n",
        "    \n",
        "    # Remove stopwords and convert to lowercase\n",
        "    clean_words = [word.lower() for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
        "    \n",
        "    return \" \".join(clean_words)\n",
        "\n",
        "# Apply the cleaning function (this might take a moment!)\n",
        "print(\"Cleaning text data... this might take a few seconds.\")\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "print(\"Text cleaning done!\")\n",
        "df[['text', 'clean_text']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Vectorization (Feature Extraction) üî¢\n",
        "Now we convert our cleaned text into numerical features using `TfidfVectorizer` (Term Frequency-Inverse Document Frequency). This downweights words that appear too frequently across all documents (like common but not 'stop' words)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=3000) # Limit to top 3000 features to keep the model lightweight\n",
        "X = tfidf.fit_transform(df['clean_text']).toarray()\n",
        "\n",
        "# Encode the target variable\n",
        "y = df['label'].map({'Spam': 1, 'Ham': 0})\n",
        "\n",
        "print(f\"Feature Matrix Shape: {X.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training ü§ñ\n",
        "We'll split our data into training and testing sets, then train two models:\n",
        "1. **Multinomial Naive Bayes**: A classic algorithm for text classification.\n",
        "2. **Support Vector Machine (SVC)**: Often performs well on high-dimensional data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training Data Shape: {X_train.shape}\")\n",
        "print(f\"Testing Data Shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_pred = nb_model.predict(X_test)\n",
        "\n",
        "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, nb_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm_model = SVC(probability=True, kernel='linear') # Linear kernel usually works best for text\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_pred = svm_model.predict(X_test)\n",
        "\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Evaluation üìä\n",
        "Let's dive deeper than just accuracy. We'll look at the Confusion Matrix and Classification Report for our best performing model (likely SVM or NB, they are usually close)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix for SVM\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, svm_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
        "            xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
        "plt.title('Confusion Matrix (SVM)', fontsize=16)\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.ylabel('Actual', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Classification Report (SVM):\\n\")\n",
        "print(classification_report(y_test, svm_pred, target_names=['Ham', 'Spam']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ROC Curve\n",
        "An ROC curve helps us visualize the trade-off between the true positive rate and false positive rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "y_prob = svm_model.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC)', fontsize=16)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion üèÅ\n",
        "We successfully built a Spam Detection model using this email dataset. \n",
        "\n",
        "- We cleaned the text data to remove noise.\n",
        "- We visualized the data to find insights (like message length).\n",
        "- We trained a powerful SVM model (and a Naive Bayes baseline).\n",
        "- The SVM model showed excellent performance with high accuracy and a strong ROC AUC score.\n",
        "\n",
        "This model could be the start of a real-world spam filter! Thanks for following along. üéâ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
